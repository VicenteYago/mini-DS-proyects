{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "773c819f",
   "metadata": {},
   "source": [
    "# 4 - Building a \"fake news\" classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcadfb8a",
   "metadata": {},
   "source": [
    "You'll apply the basics of what you've learned along with some supervised machine learning to build a \"fake news\" detector. You'll begin by learning the basics of supervised machine learning, and then move forward by choosing a few important features and testing ideas to identify and classify fake news articles.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6adcbc48",
   "metadata": {},
   "source": [
    "## Classifying fake news using supervised learning with NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff2a407",
   "metadata": {},
   "source": [
    "Supervised learning is a form of machine learning where you are given or create training data. This data has a label or outcome which you want the model or algorithm to learn. One common problem used as a good example of introductory machine learning is the Fischer's iris data; we have a few example rows of it here. The data has several features: Sepal Length and width and Petal length and width. The label we want to learn and predict is the species. This is a classification problem, so you want to be able to classify or categorize some data based on what you already know or have learned. Our goal is to use the dataset to make intelligent hypotheses about the species based on the geometric features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707f69f9",
   "metadata": {},
   "source": [
    "But instead of using geometric features like the Iris dataset, we need to use language. To help create features and train a model, we will use Scikit learn, a powerful open-source library. One of the ways you can create supervised learning data from text is by using bag of words models or TFIDF as features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a8392d",
   "metadata": {},
   "source": [
    "Let's say I have a dataset full of movie plots and genres from the IMDB database, as shown in this chart. I've separated the action and sci-fi movies, removing any movies labeled both action and scifi. I want to predict whether a movie is action or sci-fi based on the plot summary. The dataset we've extracted has categorical features generated using some preprocessing. We can see the plot summary, and the sci-fi and action columns. You can also see the Sci-Fi column, which is 1 for movies that are scifi and 0 for movies that are action. The Action column is the inverse of the Sci-Fi column.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6516ef6",
   "metadata": {},
   "source": [
    "## Building word count vectors with scikit-learn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d73a0b",
   "metadata": {},
   "source": [
    "To do so, we first need to import some necessary tools from Sci-kit learn. Once the data is loaded, we can create y which traditionally refers to the labels or outcome you want the model to learn. We can use the Sci-Fi column which has 1 if the movie is Sci-Fi and 0 if it is Action. Then, scikit learn's train_test_split function can be used to split the dataframe into training and testing data. This method will split the features which is the plot summary or column PLOT and the labels (y) based on a given test_size such as 0.33, representing 33 percent. I have also set random state so we have a repeatable result, it operates similar to setting a random seed and ensures I get the same results when I run the code again. The function will take 33% of rows to be marked as test data, and remove them from the training data. The test data is later used to see what my model has learned. The resulting data from train_test_split are training data (as X_train) and training labels (as y_train) and testing data as X_test and testing labels as y_test. \n",
    "\n",
    "- Next, we create a **countvectorizer** which turns my text into bag of words vectors similar to a Gensim corpus, it will also remove English stop words from the movie plot summaries as a preprocessing step. Each token now acts as a feature for the machine learning classification problem, just like the flower measurements in the iris data set. \n",
    "\n",
    "\n",
    "- We can then call **fit_transform** on the training data to create the **bag-of-words vectors**. Fit_transform is a handy shortcut which will call the model's fit and then transform methods; which here generates a mapping of words with IDs and vectors representing how many times each word appears in the plot. Fit_transform operates differently for each model, but generally fit will find parameters or norms in the data and transform will apply the model's underlying algorithm or approximation -- similar to preprocessing but with a specific use case in mind. \n",
    "\n",
    "\n",
    "- For the **CountVectorizer class**, **fit_transform** will create the bagofwords dictionary and vectors for each document using the training data. After calling fit_transform on the training data, we call **transform** on the test data to create bag of words vectors using the same dictionary. The training and test vectors need to use a consistent set of words, so the trained model can understand the test input. If we don't have much data, there can be an issue with words in the test set which don't appear in the training data. This will throw an error, so you will need to either add more training data or remove the unknown words from the test dataset. In only a few lines of Python, we have transformed text into bagofwords vectors and generated test and training datasets. Scikitlearn is a great aid in helping make NLP machine learning simple and accessible.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b983ba9",
   "metadata": {},
   "source": [
    "### CountVectorizer for text classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1aad694",
   "metadata": {},
   "source": [
    "t's time to begin building your text classifier! The data has been loaded into a DataFrame called df. Explore it in the IPython Shell to investigate what columns you can use. The .head() method is particularly informative.\n",
    "\n",
    "In this exercise, you'll use pandas alongside scikit-learn to create a sparse text vectorizer you can use to train and test a simple supervised model. To begin, you'll set up a CountVectorizer and investigate some of its features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "604dd5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary modules\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e407fe2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8476</td>\n",
       "      <td>You Can Smell Hillary’s Fear</td>\n",
       "      <td>Daniel Greenfield, a Shillman Journalism Fello...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10294</td>\n",
       "      <td>Watch The Exact Moment Paul Ryan Committed Pol...</td>\n",
       "      <td>Google Pinterest Digg Linkedin Reddit Stumbleu...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3608</td>\n",
       "      <td>Kerry to go to Paris in gesture of sympathy</td>\n",
       "      <td>U.S. Secretary of State John F. Kerry said Mon...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10142</td>\n",
       "      <td>Bernie supporters on Twitter erupt in anger ag...</td>\n",
       "      <td>— Kaydee King (@KaydeeKing) November 9, 2016 T...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>875</td>\n",
       "      <td>The Battle of New York: Why This Primary Matters</td>\n",
       "      <td>It's primary day in New York and front-runners...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6330</th>\n",
       "      <td>4490</td>\n",
       "      <td>State Department says it can't find emails fro...</td>\n",
       "      <td>The State Department told the Republican Natio...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6331</th>\n",
       "      <td>8062</td>\n",
       "      <td>The ‘P’ in PBS Should Stand for ‘Plutocratic’ ...</td>\n",
       "      <td>The ‘P’ in PBS Should Stand for ‘Plutocratic’ ...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6332</th>\n",
       "      <td>8622</td>\n",
       "      <td>Anti-Trump Protesters Are Tools of the Oligarc...</td>\n",
       "      <td>Anti-Trump Protesters Are Tools of the Oligar...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6333</th>\n",
       "      <td>4021</td>\n",
       "      <td>In Ethiopia, Obama seeks progress on peace, se...</td>\n",
       "      <td>ADDIS ABABA, Ethiopia —President Obama convene...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6334</th>\n",
       "      <td>4330</td>\n",
       "      <td>Jeb Bush Is Suddenly Attacking Trump. Here's W...</td>\n",
       "      <td>Jeb Bush Is Suddenly Attacking Trump. Here's W...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6335 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                                              title  \\\n",
       "0           8476                       You Can Smell Hillary’s Fear   \n",
       "1          10294  Watch The Exact Moment Paul Ryan Committed Pol...   \n",
       "2           3608        Kerry to go to Paris in gesture of sympathy   \n",
       "3          10142  Bernie supporters on Twitter erupt in anger ag...   \n",
       "4            875   The Battle of New York: Why This Primary Matters   \n",
       "...          ...                                                ...   \n",
       "6330        4490  State Department says it can't find emails fro...   \n",
       "6331        8062  The ‘P’ in PBS Should Stand for ‘Plutocratic’ ...   \n",
       "6332        8622  Anti-Trump Protesters Are Tools of the Oligarc...   \n",
       "6333        4021  In Ethiopia, Obama seeks progress on peace, se...   \n",
       "6334        4330  Jeb Bush Is Suddenly Attacking Trump. Here's W...   \n",
       "\n",
       "                                                   text label  \n",
       "0     Daniel Greenfield, a Shillman Journalism Fello...  FAKE  \n",
       "1     Google Pinterest Digg Linkedin Reddit Stumbleu...  FAKE  \n",
       "2     U.S. Secretary of State John F. Kerry said Mon...  REAL  \n",
       "3     — Kaydee King (@KaydeeKing) November 9, 2016 T...  FAKE  \n",
       "4     It's primary day in New York and front-runners...  REAL  \n",
       "...                                                 ...   ...  \n",
       "6330  The State Department told the Republican Natio...  REAL  \n",
       "6331  The ‘P’ in PBS Should Stand for ‘Plutocratic’ ...  FAKE  \n",
       "6332   Anti-Trump Protesters Are Tools of the Oligar...  FAKE  \n",
       "6333  ADDIS ABABA, Ethiopia —President Obama convene...  REAL  \n",
       "6334  Jeb Bush Is Suddenly Attacking Trump. Here's W...  REAL  \n",
       "\n",
       "[6335 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./data/fake_or_real_news.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f1fbec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0                                              title  \\\n",
      "0        8476                       You Can Smell Hillary’s Fear   \n",
      "1       10294  Watch The Exact Moment Paul Ryan Committed Pol...   \n",
      "2        3608        Kerry to go to Paris in gesture of sympathy   \n",
      "3       10142  Bernie supporters on Twitter erupt in anger ag...   \n",
      "4         875   The Battle of New York: Why This Primary Matters   \n",
      "\n",
      "                                                text label  \n",
      "0  Daniel Greenfield, a Shillman Journalism Fello...  FAKE  \n",
      "1  Google Pinterest Digg Linkedin Reddit Stumbleu...  FAKE  \n",
      "2  U.S. Secretary of State John F. Kerry said Mon...  REAL  \n",
      "3  — Kaydee King (@KaydeeKing) November 9, 2016 T...  FAKE  \n",
      "4  It's primary day in New York and front-runners...  REAL  \n"
     ]
    }
   ],
   "source": [
    "# Print the head of df\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0c290b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       FAKE\n",
       "1       FAKE\n",
       "2       REAL\n",
       "3       FAKE\n",
       "4       REAL\n",
       "        ... \n",
       "6330    REAL\n",
       "6331    FAKE\n",
       "6332    FAKE\n",
       "6333    REAL\n",
       "6334    REAL\n",
       "Name: label, Length: 6335, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a series to store the labels: y\n",
    "y = df['label']\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de66f853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['text'],y,\n",
    "                                                    test_size=0.33, random_state=53)\n",
    "\n",
    "# Initialize a CountVectorizer object: count_vectorizer\n",
    "count_vectorizer = CountVectorizer(stop_words='english')\n",
    "\n",
    "# Transform the training data using only the 'text' column values: count_train \n",
    "count_train = count_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data using only the 'text' column values: count_test \n",
    "count_test = count_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba64f693",
   "metadata": {},
   "source": [
    "[Count Vectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html):\n",
    "Convert a collection of text documents to a matrix of token counts.\n",
    "\n",
    "This implementation produces a sparse representation of the counts using scipy.sparse.csr_matrix.\n",
    "\n",
    "If you do not provide an a-priori dictionary and you do not use an analyzer that does some kind of feature selection then the number of features will be equal to the vocabulary size found by analyzing the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c8bebf0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4244x56922 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 1119820 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d5e18c63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2091x56922 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 533697 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e25a1d98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00' '000' '0000' '00000031' '000035' '00006' '0001' '0001pt' '000ft'\n",
      " '000km']\n"
     ]
    }
   ],
   "source": [
    "# Print the first 10 features of the count_vectorizer\n",
    "print(count_vectorizer.get_feature_names_out()[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab13e601",
   "metadata": {},
   "source": [
    "### TfidfVectorizer for text classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b992df09",
   "metadata": {},
   "source": [
    "Similar to the sparse CountVectorizer created in the previous exercise, you'll work on creating tf-idf vectors for your documents. You'll set up a TfidfVectorizer and investigate some of its features.\n",
    "\n",
    "In this exercise, you'll use pandas and sklearn along with the same X_train, y_train and X_test, y_test DataFrames and Series you created in the last exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9beab20c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00', '000', '0000', '00000031', '000035', '00006', '0001', '0001pt', '000ft', '000km']\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize a TfidfVectorizer object: tfidf_vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_df=0.7)\n",
    "\n",
    "# Transform the training data: tfidf_train \n",
    "tfidf_train = tfidf_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data: tfidf_test \n",
    "tfidf_test = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# Print the first 10 features\n",
    "print(tfidf_vectorizer.get_feature_names()[:10])\n",
    "\n",
    "# Print the first 5 vectors of the tfidf training data\n",
    "print(tfidf_train.A[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d19deb1",
   "metadata": {},
   "source": [
    "### Inspecting the vectors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5696d489",
   "metadata": {},
   "source": [
    "To get a better idea of how the vectors work, you'll investigate them by converting them into pandas DataFrames.\n",
    "\n",
    "Here, you'll use the same data structures you created in the previous two exercises (count_train, count_vectorizer, tfidf_train, tfidf_vectorizer) as well as pandas, which is imported as pd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5f43f01e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['00',\n",
       " '000',\n",
       " '0000',\n",
       " '00000031',\n",
       " '000035',\n",
       " '00006',\n",
       " '0001',\n",
       " '0001pt',\n",
       " '000ft',\n",
       " '000km']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer.get_feature_names()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b772d85b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['حلب', 'عربي', 'عن', 'لم', 'ما', 'محاولات', 'من', 'هذا', 'والمرضى', 'ยงade']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer.get_feature_names()[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "068c3a59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vyago/miniconda3/lib/python3.8/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   00  000  0000  00000031  000035  00006  0001  0001pt  000ft  000km  ...  \\\n",
      "0   0    0     0         0       0      0     0       0      0      0  ...   \n",
      "1   0    0     0         0       0      0     0       0      0      0  ...   \n",
      "2   0    0     0         0       0      0     0       0      0      0  ...   \n",
      "3   0    0     0         0       0      0     0       0      0      0  ...   \n",
      "4   0    0     0         0       0      0     0       0      0      0  ...   \n",
      "\n",
      "   حلب  عربي  عن  لم  ما  محاولات  من  هذا  والمرضى  ยงade  \n",
      "0    0     0   0   0   0        0   0    0        0      0  \n",
      "1    0     0   0   0   0        0   0    0        0      0  \n",
      "2    0     0   0   0   0        0   0    0        0      0  \n",
      "3    0     0   0   0   0        0   0    0        0      0  \n",
      "4    0     0   0   0   0        0   0    0        0      0  \n",
      "\n",
      "[5 rows x 56922 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vyago/miniconda3/lib/python3.8/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Create the CountVectorizer DataFrame: count_df\n",
    "count_df = pd.DataFrame(count_train.A, columns=count_vectorizer.get_feature_names())\n",
    "\n",
    "# Create the TfidfVectorizer DataFrame: tfidf_df\n",
    "tfidf_df = pd.DataFrame(tfidf_train.A, columns=tfidf_vectorizer.get_feature_names())\n",
    "\n",
    "# Print the head of count_df\n",
    "print(count_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "15cc6ba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    00  000  0000  00000031  000035  00006  0001  0001pt  000ft  000km  ...  \\\n",
      "0  0.0  0.0   0.0       0.0     0.0    0.0   0.0     0.0    0.0    0.0  ...   \n",
      "1  0.0  0.0   0.0       0.0     0.0    0.0   0.0     0.0    0.0    0.0  ...   \n",
      "2  0.0  0.0   0.0       0.0     0.0    0.0   0.0     0.0    0.0    0.0  ...   \n",
      "3  0.0  0.0   0.0       0.0     0.0    0.0   0.0     0.0    0.0    0.0  ...   \n",
      "4  0.0  0.0   0.0       0.0     0.0    0.0   0.0     0.0    0.0    0.0  ...   \n",
      "\n",
      "   حلب  عربي   عن   لم   ما  محاولات   من  هذا  والمرضى  ยงade  \n",
      "0  0.0   0.0  0.0  0.0  0.0      0.0  0.0  0.0      0.0    0.0  \n",
      "1  0.0   0.0  0.0  0.0  0.0      0.0  0.0  0.0      0.0    0.0  \n",
      "2  0.0   0.0  0.0  0.0  0.0      0.0  0.0  0.0      0.0    0.0  \n",
      "3  0.0   0.0  0.0  0.0  0.0      0.0  0.0  0.0      0.0    0.0  \n",
      "4  0.0   0.0  0.0  0.0  0.0      0.0  0.0  0.0      0.0    0.0  \n",
      "\n",
      "[5 rows x 56922 columns]\n"
     ]
    }
   ],
   "source": [
    "# Print the head of tfidf_df\n",
    "print(tfidf_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4f1a391c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set()\n"
     ]
    }
   ],
   "source": [
    "# Calculate the difference in columns: difference\n",
    "difference = set(count_df.columns) - set(tfidf_df.columns)\n",
    "print(difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1da03096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "# Check whether the DataFrames are equal\n",
    "print(count_df.equals(tfidf_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4baecebe",
   "metadata": {},
   "source": [
    "## Training and testing a classification model with scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d61f887",
   "metadata": {},
   "source": [
    "A Naive Bayes model is commonly used for testing NLP classification problems because of its basis in probability. Naive bayes algorithm uses probability, attempting to answer the question if given a particular piece of data, how likely is a particular outcome? For example, thinking back to our movie genres dataset -- If the plot has a spaceship, how likely is it that the movie is Sci-Fi? And given a Spaceship and an alien how likely NOW is it a sci-fi movie? Each word acts as a feature from our CountVectorizer helping classify our text using probability. Naive bayes has been used for text classification problems since the 1960s and continues to be used today despite the growth of many other models, algorithms and neural network architectures. That said, it is not always the best tool for the job, but it is a simple and effective one you will use to build a fake news classifier.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88a2794",
   "metadata": {},
   "source": [
    "To further evaluate our model, we can also check the confusion matrix which shows correct and incorrect labels. The confusion_matrix function from the metrics module takes the test labels, the predictions and a list of labels. If the label list is not passed, scikit learn will order them using Python ordering. The confusion matrix is a bit easier to read when we transform it into a table. The first value and last value of the matrix (or the main diagonal of the matrix) show true scores, meaning, true classification of both action and scifi films based on the plot bag of words vectors. In a confusion matrix, the predicted labels are shown across the top and the true labels are shown down the side. This confusion matrix shows 864 Sci-Fi movies incorrectly labeled as Action and 563 Action movies incorrectly labeled as Sci-Fi. We can see from the distribution of true positives and negatives that our dataset is a bit skewed, we have many more action films than sci-fi. This could be one reason that our action movies are predicted more accurately.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf08eb5",
   "metadata": {},
   "source": [
    "### Training and testing the \"fake news\" model with CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c11a4f3",
   "metadata": {},
   "source": [
    "Now it's your turn to train the \"fake news\" model using the features you identified and extracted. In this first exercise you'll train and test a Naive Bayes model using the CountVectorizer data.\n",
    "\n",
    "The training and test sets have been created, and count_vectorizer, count_train, and count_test have been computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e4bfd6b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.893352462936394\n",
      "[[ 865  143]\n",
      " [  80 1003]]\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary modules\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "\n",
    "# Instantiate a Multinomial Naive Bayes classifier: nb_classifier\n",
    "nb_classifier = MultinomialNB()\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "nb_classifier.fit(count_train, y_train)\n",
    "\n",
    "# Create the predicted tags: pred\n",
    "pred = nb_classifier.predict(count_test)\n",
    "\n",
    "# Calculate the accuracy score: score\n",
    "score = metrics.accuracy_score(y_test, pred)\n",
    "print(score)\n",
    "\n",
    "# Calculate the confusion matrix: cm\n",
    "cm = metrics.confusion_matrix(y_test, pred, labels=['FAKE', 'REAL'])\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d072dd",
   "metadata": {},
   "source": [
    "### Training and testing the \"fake news\" model with TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd254def",
   "metadata": {},
   "source": [
    "Now that you have evaluated the model using the CountVectorizer, you'll do the same using the TfidfVectorizer with a Naive Bayes model.\n",
    "\n",
    "The training and test sets have been created, and tfidf_vectorizer, tfidf_train, and tfidf_test have been computed. Additionally, MultinomialNB and metrics have been imported from, respectively, sklearn.naive_bayes and sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f26cd149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8565279770444764\n",
      "[[ 739  269]\n",
      " [  31 1052]]\n"
     ]
    }
   ],
   "source": [
    "# Create a Multinomial Naive Bayes classifier: nb_classifier\n",
    "nb_classifier = MultinomialNB()\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "nb_classifier.fit(tfidf_train, y_train)\n",
    "\n",
    "# Create the predicted tags: pred\n",
    "pred = nb_classifier.predict(tfidf_test)\n",
    "\n",
    "# Calculate the accuracy score: score\n",
    "score = metrics.accuracy_score(y_test, pred)\n",
    "print(score)\n",
    "\n",
    "# Calculate the confusion matrix: cm\n",
    "cm = metrics.confusion_matrix(y_test, pred, labels=['FAKE', 'REAL'])\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ec3fe5",
   "metadata": {},
   "source": [
    "## Simple NLP, complex problems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5a120a",
   "metadata": {},
   "source": [
    "1. **Simple NLP, complex problems**\n",
    "You've learned so much about Natural Language Processing fundamentals in this course, congratulations! In this video we'll talk more about how complex these problems can be and how to use the skills you have learned to start a longer exploration of working with language in Python. In the exercises, you will apply some extra investigation into your fake news classification model to see if it has really learned what you wanted.\n",
    "\n",
    "\n",
    "2. **Translation**\n",
    "Translation, although it might work well for some languages, still has a long way to go. This tweet by Lupin attempting to translate some legal or bureaucratic text about economics and industry is a pretty funny and also sadly accurate example of when using word vectors between two languages can end poorly. The German text uses words like Nationalökonomie and Wirtschaftswissenschaft but the English text simply says the economics of economics (including economics, economics and so forth) is part of economics. The German text has many different words related to economics and they are all simply closest to the english vector for economics, leading to a hilarious but woefully inaccurate translation.\n",
    "\n",
    "\n",
    "3. **Sentiment analysis**\n",
    "Sentiment analysis is far from a solved problem. Complex issues like snark or sarcasm, and difficult problems with negation (for example: I liked it BUT it could have been better) make it an open field of research. There is also active research regarding how separate communities use the same words differently. This is a graphic from a project called Social Sent created by a group of researchers at Stanford. The project compares sentiment changes in words over time and from different communities. Here, the authors compare sentiment in word usage between two different reddit communities, 2X which is a woman-centered reddit and sports. The graphic illustrates the SAME word can be used with very different sentiments depending on the communal understanding of the word.\n",
    "\n",
    "\n",
    "4. **Language biases**\n",
    "Finally, we must remember language can contain its own prejudices and unfair treatment towards groups. When we then train word vectors on these prejudiced texts, our word vectors will likely reflect those problems. Here, we take a gendered language, like english and translate it to Turkish, a language with no gendered pronouns. When we click to translate it back, we see the genders have switched. This phenomena was studied in a recent article by a Princeton researcher Aylin Caliskan alongside several ethical machine learning researchers. She also gave a talk at the 33rd annual Chaos Computer Club conference in Hamburg, which I can definitely recommend viewing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90244dbb",
   "metadata": {},
   "source": [
    "### Improving your model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32cef07",
   "metadata": {},
   "source": [
    "Your job in this exercise is to test a few different alpha levels using the Tfidf vectors to determine if there is a better performing combination.\n",
    "\n",
    "The training and test sets have been created, and tfidf_vectorizer, tfidf_train, and tfidf_test have been computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "66fdcdd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha:  0.0\n",
      "Score:  0.8813964610234337\n",
      "\n",
      "Alpha:  0.1\n",
      "Score:  0.8976566236250598\n",
      "\n",
      "Alpha:  0.2\n",
      "Score:  0.8938307030129125\n",
      "\n",
      "Alpha:  0.30000000000000004\n",
      "Score:  0.8900047824007652\n",
      "\n",
      "Alpha:  0.4\n",
      "Score:  0.8857006217120995\n",
      "\n",
      "Alpha:  0.5\n",
      "Score:  0.8842659014825442\n",
      "\n",
      "Alpha:  0.6000000000000001\n",
      "Score:  0.874701099952176\n",
      "\n",
      "Alpha:  0.7000000000000001\n",
      "Score:  0.8703969392635102\n",
      "\n",
      "Alpha:  0.8\n",
      "Score:  0.8660927785748446\n",
      "\n",
      "Alpha:  0.9\n",
      "Score:  0.8589191774270684\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vyago/miniconda3/lib/python3.8/site-packages/sklearn/naive_bayes.py:555: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Create the list of alphas: alphas\n",
    "alphas = np.arange(0, 1, .1)\n",
    "\n",
    "# Define train_and_predict()\n",
    "def train_and_predict(alpha):\n",
    "    # Instantiate the classifier: nb_classifier\n",
    "    nb_classifier = MultinomialNB(alpha=alpha)\n",
    "    # Fit to the training data\n",
    "    nb_classifier.fit(tfidf_train, y_train)\n",
    "    # Predict the labels: pred\n",
    "    pred = nb_classifier.predict(tfidf_test)\n",
    "    # Compute accuracy: score\n",
    "    score = metrics.accuracy_score(y_test, pred)\n",
    "    return score\n",
    "\n",
    "# Iterate over the alphas and print the corresponding score\n",
    "for alpha in alphas:\n",
    "    print('Alpha: ', alpha)\n",
    "    print('Score: ', train_and_predict(alpha))\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4674db",
   "metadata": {},
   "source": [
    "### Inspecting your model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d471dd7",
   "metadata": {},
   "source": [
    "Now that you have built a \"fake news\" classifier, you'll investigate what it has learned. You can map the important vector weights back to actual words using some simple inspection techniques.\n",
    "\n",
    "You have your well performing tfidf Naive Bayes classifier available as nb_classifier, and the vectors as tfidf_vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "df110517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the class labels: class_labels\n",
    "class_labels = nb_classifier.classes_\n",
    "\n",
    "# Extract the features: feature_names\n",
    "feature_names = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "# Zip the feature names together with the coefficient array and sort by weights: feat_with_weights\n",
    "feat_with_weights = sorted(zip(nb_classifier.coef_[0], feature_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2ce6ca13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAKE [(-11.316312804238807, '0000'), (-11.316312804238807, '000035'), (-11.316312804238807, '0001'), (-11.316312804238807, '0001pt'), (-11.316312804238807, '000km'), (-11.316312804238807, '0011'), (-11.316312804238807, '006s'), (-11.316312804238807, '007'), (-11.316312804238807, '007s'), (-11.316312804238807, '008s'), (-11.316312804238807, '0099'), (-11.316312804238807, '00am'), (-11.316312804238807, '00p'), (-11.316312804238807, '00pm'), (-11.316312804238807, '014'), (-11.316312804238807, '015'), (-11.316312804238807, '018'), (-11.316312804238807, '01am'), (-11.316312804238807, '020'), (-11.316312804238807, '023')]\n"
     ]
    }
   ],
   "source": [
    "# Print the first class label and the top 20 feat_with_weights entries\n",
    "print(class_labels[0], feat_with_weights[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "95f7fdc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REAL [(-7.742481952533027, 'states'), (-7.717550034444668, 'rubio'), (-7.703583809227384, 'voters'), (-7.654774992495461, 'house'), (-7.649398936153309, 'republicans'), (-7.6246184189367, 'bush'), (-7.616556675728881, 'percent'), (-7.545789237823644, 'people'), (-7.516447881078008, 'new'), (-7.448027933291952, 'party'), (-7.411148410203476, 'cruz'), (-7.410910239085596, 'state'), (-7.35748985914622, 'republican'), (-7.33649923948987, 'campaign'), (-7.2854057032685775, 'president'), (-7.2166878130917755, 'sanders'), (-7.108263114902301, 'obama'), (-6.724771332488041, 'clinton'), (-6.5653954389926845, 'said'), (-6.328486029596207, 'trump')]\n"
     ]
    }
   ],
   "source": [
    "# Print the second class label and the bottom 20 feat_with_weights entries\n",
    "print(class_labels[1], feat_with_weights[-20:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
