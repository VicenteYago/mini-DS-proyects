{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae2704c7",
   "metadata": {},
   "source": [
    "# Parallel Processing with Dask "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a10ed9",
   "metadata": {},
   "source": [
    "## Dask Bags for Unstructured Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a07537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Dask bag subpackage as db\n",
    "import dask.bag as db\n",
    "\n",
    "# Convert the list to a Dask bag\n",
    "review_bag = db.from_sequence(reviews_list, npartitions=3)\n",
    "\n",
    "# Print 1 element of the bag\n",
    "print(review_bag.take(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c2c2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in all the .txt files inside data/tripadvisor_hotel_reviews\n",
    "review_bag = db.read_text(\"data/tripadvisor_hotel_reviews/*.txt\")\n",
    "\n",
    "# Count the number of reviews in the bag\n",
    "review_count = review_bag.count()\n",
    "\n",
    "# Compute and print the answer\n",
    "print(review_count.compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0355b31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all of the reviews to lower case\n",
    "lowercase_reviews = review_bag.str.lower()\n",
    "\n",
    "# Count the number of times 'excellent' appears in each review\n",
    "excellent_counts = lowercase_reviews.str.count(\"excellent\")\n",
    "\n",
    "# Print the first 10 counts of 'excellent'\n",
    "print(excellent_counts.take(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5bee87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import of the json package\n",
    "import json\n",
    "\n",
    "# Read all of the JSON files inside data/politicians\n",
    "text_bag = db.read_text(\"data/politicians/*.json\")\n",
    "\n",
    "# Convert the JSON strings into dictionaries\n",
    "dict_bag = text_bag.map(json.loads)\n",
    "\n",
    "# Show an example dictionary\n",
    "print(dict_bag.take(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b861c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the number of elements in dict_bag\n",
    "print(dict_bag.count().compute())\n",
    "\n",
    "# Filter out records using the has_birth_date() function\n",
    "filtered_bag = dict_bag.filter(has_birth_date)\n",
    "\n",
    "# Print the number of elements in filtered_bag\n",
    "print(filtered_bag.count().compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbf5e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the 'birth_date' from each dictionary in the bag\n",
    "birth_date_bag = filtered_bag.pluck(\"birth_date\")\n",
    "\n",
    "# Extract the year as an integer from the birth_date strings\n",
    "birth_year_bag = birth_date_bag.map(lambda x: int(x[:4]))\n",
    "\n",
    "# Calculate the min, max and mean birth years\n",
    "min_year = birth_year_bag.min()\n",
    "max_year = birth_year_bag.max()\n",
    "mean_year = birth_year_bag.mean()\n",
    "\n",
    "# Compute the results efficiently and print them\n",
    "print(dask.compute(min_year, max_year, mean_year))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7062ff",
   "metadata": {},
   "source": [
    "### Converting unstructured data to DataFrame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ddd2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_url(x):\n",
    "    # Extract the url and assign it to the key 'url'\n",
    "    x['url'] = x['links'][0]['url']\n",
    "    return x\n",
    "  \n",
    "# Run the function on all elements in the bag.\n",
    "dict_bag = dict_bag.map(extract_url)\n",
    "\n",
    "print(dict_bag.take(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42dea6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_keys(dictionary, keys_to_keep):\n",
    "    new_dict = {}\n",
    "    # Loop through kept keys and add them to new dictionary\n",
    "    for k in keys_to_keep:\n",
    "        new_dict[k] = dictionary[k]\n",
    "    return new_dict\n",
    "\n",
    "# Use the select_keys to reduce to the 4 required keys\n",
    "filtered_bag = dict_bag.map(select_keys, keys_to_keep=['gender','name', 'birth_date', 'url'])\n",
    "\n",
    "# Convert the restructured bag to a DataFrame\n",
    "df = filtered_bag.to_dataframe()\n",
    "\n",
    "# Print the first few rows of the DataFrame\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a44b5b",
   "metadata": {},
   "source": [
    "### Using any data in Dask bags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d650bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import scipy module for .wav files\n",
    "import numpy as np\n",
    "from scipy.io import wavfile\n",
    "import dask \n",
    "import dask.bag as db\n",
    "\n",
    "def load_wav(filename):\n",
    "    # Load in the audio data\n",
    "    sampling_freq, audio = wavfile.read(filename)\n",
    "    \n",
    "    # Add the filename, audio data, and sampling frequency to the dictionary\n",
    "    data_dict = {\n",
    "        'filename': filename,\n",
    "        'audio': audio, \n",
    "        'sample_frequency': sampling_freq\n",
    "    }\n",
    "    return data_dict\n",
    "\n",
    "def not_silent(data_dict):\n",
    "    # Check if the audio data is silent\n",
    "    return np.mean(np.abs(data_dict['audio'])) > 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2dc6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the list of filenames into a Dask bag\n",
    "filename_bag = db.from_sequence(wavfiles)\n",
    "\n",
    "# Apply the load_wav() function to each element of the bag\n",
    "loaded_audio_bag = filename_bag.map(load_wav)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b612807a",
   "metadata": {},
   "source": [
    "Alternative version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341ea38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "delayed_loaded_audio = []\n",
    "\n",
    "for wavfile in wavfiles:\n",
    "    # Append the delayed loaded audio to the list\n",
    "    delayed_loaded_audio.append(dask.delayed(load_wav)(wavfile))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445acaa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the list to a Dask bag\n",
    "loaded_audio_bag = db.from_delayed(delayed_loaded_audio)\n",
    "\n",
    "# Filter out blank audio files\n",
    "filtered_audio_bag = loaded_audio_bag.filter(not_silent)\n",
    "\n",
    "# Apply the peak_frequency function to all audio files\n",
    "audio_and_freq_bag = filtered_audio_bag.map(peak_frequency)\n",
    "\n",
    "# Use the delete_dictionary_entry function to drop the audio\n",
    "final_bag = audio_and_freq_bag.map(delete_dictionary_entry, key_to_drop='audio')\n",
    "\n",
    "# Convert to a DataFrame and run the computation\n",
    "df = final_bag.to_dataframe().compute()\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8cb507",
   "metadata": {},
   "source": [
    "Nice work! An important part of this calculation was removing the audio data after we had performed our calcuations on it. The audio data is much larger than the information we actually want to extract from it, so it is important to drop it before we run the compute method. You might notice that the notes and frequencies here are the 6 standard guitar strings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631c129d",
   "metadata": {},
   "source": [
    "## Dask ML and final pieces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a8bcec",
   "metadata": {},
   "source": [
    "### Using processes and threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30aa0ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Client and LocalCluster\n",
    "from dask.distributed import Client, LocalCluster\n",
    "\n",
    "# Create a thread-based local cluster\n",
    "cluster = LocalCluster(\n",
    "\tprocesses=False,\n",
    "    n_workers=4,\n",
    "    threads_per_worker=1,\n",
    ")\n",
    "\n",
    "# Create a client\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd26b3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "\n",
    "# Create a client without creating cluster first\n",
    "client = Client(\n",
    "\tprocesses=False, \n",
    "    n_workers=4,\n",
    "    threads_per_worker=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ff0ea8",
   "metadata": {},
   "source": [
    "### Training ML models on big datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f186c1d9",
   "metadata": {},
   "source": [
    "The input variables are available as dask_X and contain a few numeric columns, such as the song's tempo and danceability. The target values are available as dask_y and are the popularity score of each song."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c485628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the SGDRegressor and the Incremental wrapper\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from dask_ml.wrappers import Incremental\n",
    "\n",
    "# Create a SGDRegressor model\n",
    "model = SGDRegressor()\n",
    "\n",
    "# Wrap the model so that it works with Dask\n",
    "dask_model = Incremental(model, scoring = \"neg_mean_squared_error\")\n",
    "\n",
    "# Fit the wrapped model\n",
    "dask_model.fit(dask_X, dask_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125ddb45",
   "metadata": {},
   "source": [
    "Great work! Whenever you run the .fit() method, Dask optimizes the computation by copying the model to the process or thread where the data is, rather than copying the data into the main process which holds the model. It can take a long time to copy information, and the model is much smaller than the dataset, so this is much more efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb9dc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over the training data 5 times\n",
    "for i in range(5):\n",
    "\tdask_model.partial_fit(dask_X,dask_y)\n",
    "\n",
    "# Use your model to make predictions\n",
    "y_pred_delayed = dask_model.predict(dask_X)\n",
    "\n",
    "# Compute the predictions\n",
    "y_pred_computed = y_pred_delayed.compute()\n",
    "\n",
    "print(y_pred_computed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdf16a7",
   "metadata": {},
   "source": [
    "Fantastic! That is a well-fit model. If you were just to use the .fit() method 5 times, your code would run, but you wouldn't get more accurate predictions on each loop repetition. When .fit() is run, the model is reset back to an unfitted state and refit to the data, so you start from scratch each time. Using the .partial_fit() method allows us to pick up fitting from where we left off and refine the previous loop's fitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f60a74",
   "metadata": {},
   "source": [
    "### Preprocessing big datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e80491",
   "metadata": {},
   "source": [
    "#### Lazily transforming training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5930f4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the StandardScaler class\n",
    "from dask_ml.preprocessing import StandardScaler\n",
    "\n",
    "X = dask_df[['duration_ms', 'explicit', 'danceability', 'acousticness', 'instrumentalness', 'tempo']]\n",
    "\n",
    "# Select the target variable\n",
    "y = dask_df[['popularity']]\n",
    "\n",
    "# Create a StandardScaler object and fit it on X\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "\n",
    "# Transform X\n",
    "X = scaler.transform(X)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d97394",
   "metadata": {},
   "source": [
    "Well done! You may have noticed that X is still a Dask DataFrame even after being transformed. However, you have already had to load all the data in X once so that you could fit the scaler."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ac091c",
   "metadata": {},
   "source": [
    "#### Lazy train-test split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46cf5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the train_test_split function\n",
    "from dask_ml.model_selection import train_test_split\n",
    "\n",
    "# Rescale the target values\n",
    "y = y / 100\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True, test_size=0.2)\n",
    "\n",
    "print(X_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
